{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be careful that your input voices must be clear and without any amount of noise or music or silence. They should be voice of only one person (because of some TTS models conditions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import speech_recognition as sr\n",
    "import pydub\n",
    "from pydub import AudioSegment\n",
    "import wave\n",
    "import io\n",
    "import os\n",
    "import soundfile as sf\n",
    "import scipy.signal as signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "durs is a list of lists which contains durations of target voice in each file in miliseconds. for example if in first file i want to trim the file from 00:00:12 to 01:05:03 then my first element should be: [12x1000:1x60x60x1000+5x60x1000+3x1000] in other way [12000:3903000]. \n",
    "\n",
    "here is a real example for three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durs = [[53893,1028903],[15429,18*60000+35068],[13961,17*60000+28160]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we give the duration of voice files to code and get resampled single-channel audio chunks with 200 ms silence at the start and end of each chunk. the code trims the voice in silence parts so you won't get chunks that have incomplete words. I've set the minimum chunk duration to 7 seconds and the maximum to 10 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('data')\n",
    "os.mkdir('data/wavs')\n",
    "\n",
    "directo = \"<directory to voice files>\"\n",
    "\n",
    "target_sample_rate = 24000\n",
    "\n",
    "voice_names = os.listdir(directo)\n",
    "voice_names.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "sound = AudioSegment.silent(duration=200)\n",
    "\n",
    "\n",
    "for voicename, x in zip(voice_names, durs):\n",
    "\n",
    "  voice = AudioSegment.from_file(directo+\"/\"+voicename)\n",
    "  sound += voice[x[0]:x[1]]\n",
    "  # Be careful about the RAM capacity. If using all voices at once takes high values of RAM, then try list slicing (divide it into smaller parts).\n",
    "  # For example once from voice 1 till 8 and then from voice 8 till last one.\n",
    "\n",
    "\n",
    "sound.set_channels(1)\n",
    "\n",
    "min_len = 7*1000\n",
    "max_len = min_len + 5*1000\n",
    "\n",
    "start = 0\n",
    "end = 0\n",
    "\n",
    "silence_2 = AudioSegment.silent(duration=200)\n",
    "\n",
    "i = 0\n",
    "\n",
    "while end < len(sound):\n",
    "\n",
    "    audio_chunks = split_on_silence(sound[start + min_len:start + max_len], min_silence_len=200, silence_thresh=-47, keep_silence=0)\n",
    "    output_file = f\"data/wavs/{i}.wav\"\n",
    "    print(\"Exporting file\", output_file)\n",
    "    end = start + min_len + len(audio_chunks[0])\n",
    "    final_file = (silence_2 + sound[start:end] + silence_2).set_channels(1)\n",
    "    final_file.export(output_file, format=\"wav\")\n",
    "\n",
    "    data, samplerate = sf.read(output_file)\n",
    "    resampled_data = signal.resample(data, int(len(data) * target_sample_rate / samplerate))\n",
    "    sf.write(output_file, resampled_data, target_sample_rate, subtype='PCM_24')\n",
    "    print(\"Resampling file\", output_file)\n",
    "    start = end\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to remove the files (chunks) with a length of less than 7 seconds, to delete chunks of end of voices which usually contain nothing because of their short length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/wavs\"\n",
    "files_names = os.listdir(directory)\n",
    "files_names.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "for filename in files_names:\n",
    "  with wave.open(directory+\"/\"+filename) as mywav2:\n",
    "        duration = mywav2.getnframes() / mywav2.getframerate()\n",
    "\n",
    "  if duration < 7:\n",
    "    os.remove(directory+\"/\"+filename)\n",
    "    print(f\"{directory+'/'+filename} with {duration} length removed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's multiprocess the function that uses audio chunks to write the relevant transcription for each one in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "directory = \"data/wavs\"\n",
    "\n",
    "files_names = os.listdir(directory)\n",
    "files_names.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "meta = io.open(r\"data/metadata.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "def fun(part):\n",
    "\n",
    "  file_names = files_names[part[0]:part[1]]\n",
    "\n",
    "  recognizer = sr.Recognizer()\n",
    "\n",
    "  for filename in file_names:\n",
    "\n",
    "      with sr.AudioFile(directory+\"/\"+filename) as source:\n",
    "          audio = recognizer.record(source)\n",
    "\n",
    "      text = recognizer.recognize_google(audio, language='fa-IR')\n",
    "\n",
    "      meta.write(f\"{text}|{filename}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "  pool = multiprocessing.Pool()\n",
    "\n",
    "  inputs = [[0, 500],[500, 1000],[1000, 1500],[1500, 2000],[2000, 2500],[2500,len(files_names)]]\n",
    "  # I've used \"input\" as a list to process the first 500 chunks in a single process and the second 500 chunks in another process and so on.\n",
    "  # Here I have 6 parallel processes.(6 elements in list)\n",
    "  # (multiprocess is very useful here to boost the code because every single chunk takes some time to get processed on \n",
    "  # the internet and generate the transcription).\n",
    "\n",
    "  pool = multiprocessing.Pool(processes=len(inputs))\n",
    "\n",
    "  pool.map(fun, inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of multiprocessing, it's possible to lose some data, so we compare transcriptions with audio chunks and generate transcriptions for the audio chunks that don't have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "meta = io.open(r\"data/metadata.csv\", mode=\"r\", encoding=\"utf-8\")\n",
    "files_names = os.listdir(\"data/wavs/\")\n",
    "files_names.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "\n",
    "list_wavs = []\n",
    "list_txt = []\n",
    "\n",
    "for filename in files_names:\n",
    "    list_wavs.append(filename)\n",
    "\n",
    "\n",
    "for txt in meta.readlines():\n",
    "    list_txt.append(txt.split(\"|\")[1][:-1])\n",
    "\n",
    "def uncommon_elements(list1, list2):\n",
    "    result = []\n",
    "    for element in list1:\n",
    "        if element not in list2:\n",
    "            result.append(element)\n",
    "    return result\n",
    "\n",
    "unc_elements = uncommon_elements(list_wavs, list_txt)\n",
    "print(unc_elements)\n",
    "meta.close()\n",
    "\n",
    "meta = io.open(r\"data/metadata.csv\", mode=\"a\", encoding=\"utf-8\")\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "for filename in unc_elements:\n",
    "\n",
    "    with sr.AudioFile(\"data/wavs/\"+filename) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    print(filename)\n",
    "    text = recognizer.recognize_google(audio, language='fa-IR')\n",
    "\n",
    "    meta.write(f\"{text}|{filename}\\n\")\n",
    "\n",
    "\n",
    "meta.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not necessary but we can sort the transcriptions using their related audio chunk name using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "lines_seen = set()\n",
    "\n",
    "outfile = io.open(r\"metadata.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "for line in io.open(r\"metadataAllAll.csv\", mode=\"r\", encoding=\"utf-8\"):\n",
    "\n",
    "    if line not in lines_seen:\n",
    "\n",
    "        outfile.write(line)\n",
    "        lines_seen.add(line)\n",
    "\n",
    "outfile.close()\n",
    "\n",
    "\n",
    "def my_sort(line):\n",
    "\tline_fields = line.strip().split('|')\n",
    "\tindx = float(line_fields[1][:-4])\n",
    "\treturn indx\n",
    "\n",
    "\n",
    "fp = io.open(r\"metadata.csv\", mode=\"r\", encoding=\"utf-8\")\n",
    "contents = fp.readlines()\n",
    "fp.close()\n",
    "\n",
    "fp = io.open(r\"metadata.csv\", mode=\"w\", encoding=\"utf-8\")\n",
    "\n",
    "contents.sort(key=my_sort)\n",
    "\n",
    "for line in contents:\n",
    "\tfp.write(line)\n",
    "\t\n",
    "fp.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
